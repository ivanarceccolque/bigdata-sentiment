AnÃ¡lisis de Sentimiento en Tweets  â€” Pipeline de Big Data
Este proyecto implementa un pipeline de Big Data para procesar tweets en espaÃ±ol y analizar el sentimiento expresado en cada uno. Utiliza herramientas modernas como Apache Kafka, PySpark y MongoDB Atlas, y estÃ¡ diseÃ±ado para ejecutarse completamente en Google Colab.

Resumen del Pipeline
El flujo de datos sigue esta secuencia:
- Ingesta: Lectura de tweets desde un archivo CSV
- SimulaciÃ³n de streaming: EnvÃ­o de datos mediante Kafka
- Procesamiento distribuido: Limpieza, codificaciÃ³n y enriquecimiento con PySpark
- Almacenamiento NoSQL: InserciÃ³n de datos procesados en MongoDB Atlas
- ExploraciÃ³n visual: ValidaciÃ³n de calidad y distribuciÃ³n de datos en notebooks

ğŸ“ Estructura del Proyecto
bigdata-sentiment/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tweets.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â”œâ”€â”€ process_tweets.py
â”‚   â””â”€â”€ mongo_writer_pymongo.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”œâ”€â”€ config/
â”‚   â””â”€â”€ spark_config.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸš€ EjecuciÃ³n del Proyecto en Google Colab
1. Abrir un notebook en Colab
- Ve a Google Colab
- Crea un nuevo notebook
- RenÃ³mbralo como pipeline_sentiment_analysis.ipynb
2. Cargar la estructura del proyecto
- Sube los archivos y carpetas del proyecto a tu entorno de Colab
- AsegÃºrate de que el archivo tweets.csv estÃ© en la carpeta /content/data/
3. Instalar dependencias
- Instala Java y las librerÃ­as necesarias usando comandos en celdas
- Configura la variable de entorno JAVA_HOME para que PySpark funcione correctamente
4. Importar librerÃ­as y configurar Spark
- Importa pyspark, pandas, matplotlib, seaborn, pymongo, y kafka-python
- Crea una sesiÃ³n de Spark con nombre personalizado
5. Cargar y explorar el dataset
- Usa pandas para visualizar las primeras filas del archivo CSV
- Ejecuta el notebook exploratory_analysis.ipynb para ver:
- DistribuciÃ³n de sentimientos
- Limpieza de texto
- Longitud de tweets
- Palabras frecuentes
6. Procesar los tweets con PySpark
- Ejecuta el script process_tweets.py o copia su contenido en una celda
- Este script realiza:
- Limpieza de texto
- TokenizaciÃ³n
- CodificaciÃ³n de sentimiento
- Enriquecimiento con fecha y longitud
7. Guardar los datos en MongoDB Atlas

ğŸ” Crear una cuenta en MongoDB Atlas
- Ve a MongoDB Atlas
- RegÃ­strate y crea un clÃºster gratuito
- Crea una base de datos llamada tweets_db y una colecciÃ³n llamada processed
- En Network Access, agrega la IP 0.0.0.0/0 para permitir acceso desde Colab
- En Database Access, crea un usuario con permisos de lectura y escritura
ğŸ”— Obtener tu URI de conexiÃ³n
- Copia el URI de conexiÃ³n que MongoDB Atlas te proporciona
- Reemplaza <password> por tu contraseÃ±a real
- AsegÃºrate de incluir el nombre de la base de datos en el URI
ğŸ—ƒï¸ Ejecutar el script de almacenamiento
- Ejecuta el script mongo_writer_pymongo.py en una celda
- Este script:
- Verifica la conexiÃ³n con MongoDB Atlas
- Convierte los datos procesados en diccionarios
- Inserta los documentos en la colecciÃ³n processed

âœ… Requisitos
- Python 3.10+
- Google Colab o entorno local con PySpark
- Cuenta activa en MongoDB Atlas
- ConexiÃ³n a internet

ğŸ“š CrÃ©ditos
- Dataset: Kaggle â€” Tweets en espaÃ±ol con etiquetas de sentimiento
- Herramientas: Apache Spark, Kafka, MongoDB Atlas, PyMongo, Google Colab
- Autor: IvÃ¡n Arce Ccolque

codigo directo en colab.  https://colab.research.google.com/drive/1GOqFjFuMOEHC6Box-DS5zBlfJ3_13g6L?usp=sharing
